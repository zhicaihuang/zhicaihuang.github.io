---
layout: post
comments: False
title: "LSTM-CRF for Named-Entity Recognition task"
excerpt: "LSTM-CRF NER task"
date:   2019-12-25 14:19:34
mathjax: true
---

##  Named-Entity Recognition (NER)
### 1 What is NER?
Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) 
is a subtask of information extraction that seeks to locate and classify named entity mentions 
in unstructured text into pre-defined categories such as person names, organizations, 
locations, medical codes, time expressions, quantities, monetary values, percentages, etc. [[1]](#1)
NER is subfield of NLP and IR(information retrieve).

### 2 Solution of NER
There is many tools can be done with NER task, such as Spacy and Stanford NLP tool.
In Stanford NLP tool, the NER recognizer realized by a linear-chain CRF model.
The CRF model is conditional random field model, it is a sequential model, it can be seem as
the evolution of traditional HMM (hidden markov model), owing to it break the two basis markov hypothesis:

- Observation independence
- First-order Homogeneous Markov property

Here we talking about how LSTM-CRF method to solve NER task. this paper mainly refer code of [PythonWorkshop](https://github.com/PythonWorkshop/intro-to-nlp-with-pytorch). It use the LSTM and CRF algorithm dealing the NER task.

### 3 LSTM+CRF
As we know, a traditional machine learning including two vital process: learning and inference.
First, learning the model, then using learned model to inference the unseen data.
#### 3.1 Learning
CRF method need state transform information and emission information. but how can we get such stuff,
the code shows that it use the LSTM model get the features of input sequence, and dimension of feature is the same as tag size.
example the tag containing B(egining), I(nside), O(utside), Start, End, five state, so the dimension LSTM output feature would be 5.
These LSTM output feature would be seem as emission probability from state to observation data.

For CRF, the other requisite information is state transform probability. 
The code shows that this transform matrix is initialized randomly.
In next step, it will learned with LSTM model.

Finally, the most two information transform probability and emission probability are done, then with both, it can get the log-sum-exp of LSTM model, which as the measure method.
the log-sum-exp can be seen as the denominator of softmax formula, which capture the global information of sequence feature.
$$
logSumExp(x_1,...,x_n)=log(\sum_{i=1}^n{e^{x_i}})
$$


#### 3.2 Inference
Without the CRF, the LSTM is able to solve NER task. It can calculate the maximum value of each observation then get the best state.
However, the result sequence may not be reasonable. such as state sequence O-I, which it is impossible happen.
So utilize the CRF to constrain the global information for LSTM, so that make the state output reasonable.

In inference, the code apply Viterbi algorithm to get the best state sequence. Viterbi code is as following [2][3]: 

```python
def _viterbi_decode(self, feats):
	backpointers = []

	# Initialize the viterbi variables in log space
	init_vvars = torch.full((1, self.tagset_size), -10000.)
	init_vvars[0][self.tag_to_ix[START_TAG]] = 0

	# forward_var at step i holds the viterbi variables for step i-1
	forward_var = init_vvars
	for feat in feats:
		bptrs_t = []  # holds the backpointers for this step
		viterbivars_t = []  # holds the viterbi variables for this step

		for next_tag in range(self.tagset_size):
			# next_tag_var[i] holds the viterbi variable for tag i at the
			# previous step, plus the score of transitioning
			# from tag i to next_tag.
			# We don't include the emission scores here because the max
			# does not depend on them (we add them in below)

			# --just plus, not traditional probability multiply.--
			next_tag_var = forward_var + self.transitions[next_tag]
			best_tag_id = argmax(next_tag_var)
			bptrs_t.append(best_tag_id)
			viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))

		# Now add in the emission scores, and assign forward_var to the set
		# of viterbi variables we just computed
		forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)
		backpointers.append(bptrs_t)

	# Transition to STOP_TAG
	terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]
	best_tag_id = argmax(terminal_var)
	path_score = terminal_var[0][best_tag_id]

	# Follow the back pointers to decode the best path.

	# --from the best end index, trace back to start index--
	# --during this process, best path will get--
	best_path = [best_tag_id]
	for bptrs_t in reversed(backpointers):
		best_tag_id = bptrs_t[best_tag_id]
		best_path.append(best_tag_id)

	# Pop off the start tag (we dont want to return that to the caller)
	start = best_path.pop()
	assert start == self.tag_to_ix[START_TAG]  # Sanity check
	best_path.reverse()
	return path_score, best_path
```


### Reference
[1] [WiKi of Named-entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition)  
[2] [pytorch tutorial: ADVANCED: MAKING DYNAMIC DECISIONS AND THE BI-LSTM CRF](https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#sphx-glr-beginner-nlp-advanced-tutorial-py)  
[3] [pytorch lstm crf](https://blog.csdn.net/Jason__Liang/article/details/81772632)

